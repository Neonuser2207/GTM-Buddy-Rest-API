{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXvtlRJEVpXy",
        "outputId": "7a0ff018-151d-4bd8-d99a-3b69f5df18e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic dataset generated and saved as 'calls_dataset.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Define possible labels\n",
        "labels = [\"Objection\", \"Pricing Discussion\", \"Security\", \"Competition\", \"Positive\"]\n",
        "\n",
        "# Define some common phrases for each label\n",
        "label_phrases = {\n",
        "    \"Objection\": [\n",
        "        \"The customer raised an objection about\",\n",
        "        \"They expressed concerns regarding\",\n",
        "        \"There was pushback on\",\n",
        "        \"They were hesitant about\",\n",
        "    ],\n",
        "    \"Pricing Discussion\": [\n",
        "        \"They asked about the pricing model\",\n",
        "        \"The discussion revolved around the cost\",\n",
        "        \"They inquired about discounts\",\n",
        "        \"The budget was a key concern\",\n",
        "    ],\n",
        "    \"Security\": [\n",
        "        \"They asked about SOC2 certification\",\n",
        "        \"Data handling was a major concern\",\n",
        "        \"They wanted to know about compliance\",\n",
        "        \"Security protocols were discussed\",\n",
        "    ],\n",
        "    \"Competition\": [\n",
        "        \"They mentioned CompetitorX as an alternative\",\n",
        "        \"CompetitorY was brought up during the call\",\n",
        "        \"They compared the product to CompetitorZ\",\n",
        "        \"CompetitorX was highlighted as a cheaper option\",\n",
        "    ],\n",
        "    \"Positive\": [\n",
        "        \"They appreciated the analytics feature\",\n",
        "        \"The AI engine was praised\",\n",
        "        \"They were impressed with the data pipeline\",\n",
        "        \"They loved the product's ease of use\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Generate synthetic data\n",
        "data = []\n",
        "for i in range(1, 101):  # Generate 100 rows\n",
        "    selected_labels = random.sample(labels, k=random.randint(1, 3))  # Randomly select 1-3 labels\n",
        "    text_snippet = \"\"\n",
        "\n",
        "    # Build the text snippet based on selected labels\n",
        "    for label in selected_labels:\n",
        "        text_snippet += random.choice(label_phrases[label]) + \" \"\n",
        "\n",
        "    # Add the row to the dataset\n",
        "    data.append({\n",
        "        \"id\": i,\n",
        "        \"text_snippet\": text_snippet.strip(),\n",
        "        \"labels\": \", \".join(selected_labels)\n",
        "    })\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"calls_dataset.csv\", index=False)\n",
        "print(\"Synthetic dataset generated and saved as 'calls_dataset.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"calls_dataset.csv\")\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Function to clean and preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "\n",
        "    # Remove stop words\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    # Lemmatize the text\n",
        "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the text snippets\n",
        "df[\"cleaned_text\"] = df[\"text_snippet\"].apply(preprocess_text)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "df.to_csv(\"cleaned_calls_dataset.csv\", index=False)\n",
        "print(\"Text preprocessing complete. Cleaned dataset saved as 'cleaned_calls_dataset.csv'.\")"
      ],
      "metadata": {
        "id": "c98ZaLvBfCvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa3c0741-59fe-4117-d4bf-a9adf0b95e60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text preprocessing complete. Cleaned dataset saved as 'cleaned_calls_dataset.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the splits\n",
        "train_df.to_csv(\"train_dataset.csv\", index=False)\n",
        "test_df.to_csv(\"test_dataset.csv\", index=False)\n",
        "print(\"Dataset split into training and test sets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqbSYinafu4_",
        "outputId": "dd116a0f-d869-4d21-ecdc-3fbf65972183"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split into training and test sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import pandas as pd\n",
        "\n",
        "# Load the training dataset\n",
        "train_df = pd.read_csv(\"train_dataset.csv\")\n",
        "\n",
        "# Convert labels to binary format\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train = mlb.fit_transform(train_df[\"labels\"].str.split(\", \"))\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train = vectorizer.fit_transform(train_df[\"cleaned_text\"])\n",
        "\n",
        "# Train the model\n",
        "model = MultiOutputClassifier(LogisticRegression())\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w93UTAfqfzSl",
        "outputId": "c29d35a5-b033-439e-e27b-6be49e7fe612"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the test dataset\n",
        "test_df = pd.read_csv(\"test_dataset.csv\")\n",
        "\n",
        "# Convert labels to binary format\n",
        "y_test = mlb.transform(test_df[\"labels\"].str.split(\", \"))\n",
        "\n",
        "# Vectorize the test data\n",
        "X_test = vectorizer.transform(test_df[\"cleaned_text\"])\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred, target_names=mlb.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpEVgQKCf5Xm",
        "outputId": "98ddea35-28ae-4196-aa5b-73b37c313186"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "       Competition       1.00      1.00      1.00         6\n",
            "         Objection       1.00      1.00      1.00         6\n",
            "          Positive       1.00      1.00      1.00         8\n",
            "Pricing Discussion       1.00      1.00      1.00         9\n",
            "          Security       1.00      1.00      1.00        11\n",
            "\n",
            "         micro avg       1.00      1.00      1.00        40\n",
            "         macro avg       1.00      1.00      1.00        40\n",
            "      weighted avg       1.00      1.00      1.00        40\n",
            "       samples avg       1.00      1.00      1.00        40\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define domain knowledge\n",
        "domain_knowledge = {\n",
        "    \"competitors\": [\"CompetitorX\", \"CompetitorY\", \"CompetitorZ\"],\n",
        "    \"features\": [\"analytics\", \"AI engine\", \"data pipeline\"],\n",
        "    \"pricing_keywords\": [\"discount\", \"renewal cost\", \"budget\", \"pricing model\"]\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "with open(\"domain_knowledge.json\", \"w\") as f:\n",
        "    json.dump(domain_knowledge, f, indent=4)\n",
        "\n",
        "print(\"Domain knowledge saved as 'domain_knowledge.json'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paDoWsP7hzV5",
        "outputId": "7d3c297d-37d4-435d-cb29-b614cd62e03b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Domain knowledge saved as 'domain_knowledge.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load domain knowledge\n",
        "with open(\"domain_knowledge.json\", \"r\") as f:\n",
        "    domain_knowledge = json.load(f)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"calls_dataset.csv\")\n",
        "\n",
        "# Function to perform dictionary lookup\n",
        "def dictionary_lookup(text):\n",
        "    entities = {\n",
        "        \"competitors\": [],\n",
        "        \"features\": [],\n",
        "        \"pricing_keywords\": []\n",
        "    }\n",
        "\n",
        "    # Search for competitors\n",
        "    for competitor in domain_knowledge[\"competitors\"]:\n",
        "        if competitor.lower() in text.lower():\n",
        "            entities[\"competitors\"].append(competitor)\n",
        "\n",
        "    # Search for features\n",
        "    for feature in domain_knowledge[\"features\"]:\n",
        "        if feature.lower() in text.lower():\n",
        "            entities[\"features\"].append(feature)\n",
        "\n",
        "    # Search for pricing keywords\n",
        "    for keyword in domain_knowledge[\"pricing_keywords\"]:\n",
        "        if keyword.lower() in text.lower():\n",
        "            entities[\"pricing_keywords\"].append(keyword)\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Apply dictionary lookup to the dataset\n",
        "df[\"extracted_entities\"] = df[\"text_snippet\"].apply(dictionary_lookup)\n",
        "\n",
        "# Save the results\n",
        "df.to_csv(\"calls_dataset_with_entities.csv\", index=False)\n",
        "print(\"Dictionary lookup complete. Results saved as 'calls_dataset_with_entities.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LINb-6dWh1Zp",
        "outputId": "de8a804e-be4b-4d67-e104-6c0c8a4a7233"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary lookup complete. Results saved as 'calls_dataset_with_entities.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform NER\n",
        "def perform_ner(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {\n",
        "        \"organizations\": [],\n",
        "        \"dates\": [],\n",
        "        \"locations\": []\n",
        "    }\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"ORG\":\n",
        "            entities[\"organizations\"].append(ent.text)\n",
        "        elif ent.label_ == \"DATE\":\n",
        "            entities[\"dates\"].append(ent.text)\n",
        "        elif ent.label_ == \"GPE\":\n",
        "            entities[\"locations\"].append(ent.text)\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Apply NER to the dataset\n",
        "df[\"ner_entities\"] = df[\"text_snippet\"].apply(perform_ner)\n",
        "\n",
        "# Save the results\n",
        "df.to_csv(\"calls_dataset_with_ner.csv\", index=False)\n",
        "print(\"NER complete. Results saved as 'calls_dataset_with_ner.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTUWiUY3h6hB",
        "outputId": "af62e237-34eb-4329-b094-23789d1b5380"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER complete. Results saved as 'calls_dataset_with_ner.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine dictionary lookup and NER results\n",
        "df[\"final_entities\"] = df.apply(\n",
        "    lambda row: {\n",
        "        \"dictionary_lookup\": row[\"extracted_entities\"],\n",
        "        \"ner_entities\": row[\"ner_entities\"]\n",
        "    },\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Save the final results\n",
        "df.to_csv(\"calls_dataset_with_final_entities.csv\", index=False)\n",
        "print(\"Entity extraction complete. Final results saved as 'calls_dataset_with_final_entities.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcYA4CI3iIDu",
        "outputId": "2620659a-007e-4986-953e-6bbe4c01d3be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity extraction complete. Final results saved as 'calls_dataset_with_final_entities.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"fastapi[standard]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoX5lVHytqny",
        "outputId": "20f2f936-2a97-4d19-f71c-c3a4d198a6e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi[standard]\n",
            "  Downloading fastapi-0.115.7-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi[standard])\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (2.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (4.12.2)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (0.28.1)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (3.1.5)\n",
            "Collecting python-multipart>=0.0.18 (from fastapi[standard])\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard])\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard])\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]) (3.10)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.15.1)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading rich_toolkit-0.13.2-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.5->fastapi[standard]) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (2.27.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (8.1.8)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (6.0.2)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (14.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->fastapi[standard]) (1.3.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.1.2)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.7-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading rich_toolkit-0.13.2-py3-none-any.whl (13 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, uvicorn, python-multipart, python-dotenv, httptools, dnspython, watchfiles, starlette, email-validator, rich-toolkit, fastapi, fastapi-cli\n",
            "Successfully installed dnspython-2.7.0 email-validator-2.2.0 fastapi-0.115.7 fastapi-cli-0.0.7 httptools-0.6.4 python-dotenv-1.0.1 python-multipart-0.0.20 rich-toolkit-0.13.2 starlette-0.45.3 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import json\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import numpy as np\n",
        "\n",
        "# Load domain knowledge\n",
        "with open(\"domain_knowledge.json\", \"r\") as f:\n",
        "    domain_knowledge = json.load(f)\n",
        "\n",
        "# Load the spaCy model for NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the trained multi-label classification model (dummy model for demonstration)\n",
        "# In a real scenario, you would load a pre-trained model here.\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "mlb = MultiLabelBinarizer()\n",
        "model = MultiOutputClassifier(LogisticRegression())\n",
        "\n",
        "# Dummy training data (replace with actual training data)\n",
        "train_df = pd.read_csv(\"train_dataset.csv\")\n",
        "y_train = mlb.fit_transform(train_df[\"labels\"].str.split(\", \"))\n",
        "X_train = vectorizer.fit_transform(train_df[\"cleaned_text\"])\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Define input model\n",
        "class Snippet(BaseModel):\n",
        "    text: str\n",
        "\n",
        "# Function for dictionary lookup\n",
        "def dictionary_lookup(text):\n",
        "    entities = {\n",
        "        \"competitors\": [],\n",
        "        \"features\": [],\n",
        "        \"pricing_keywords\": []\n",
        "    }\n",
        "\n",
        "    for competitor in domain_knowledge[\"competitors\"]:\n",
        "        if competitor.lower() in text.lower():\n",
        "            entities[\"competitors\"].append(competitor)\n",
        "\n",
        "    for feature in domain_knowledge[\"features\"]:\n",
        "        if feature.lower() in text.lower():\n",
        "            entities[\"features\"].append(feature)\n",
        "\n",
        "    for keyword in domain_knowledge[\"pricing_keywords\"]:\n",
        "        if keyword.lower() in text.lower():\n",
        "            entities[\"pricing_keywords\"].append(keyword)\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Function for NER\n",
        "def perform_ner(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {\n",
        "        \"organizations\": [],\n",
        "        \"dates\": [],\n",
        "        \"locations\": []\n",
        "    }\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"ORG\":\n",
        "            entities[\"organizations\"].append(ent.text)\n",
        "        elif ent.label_ == \"DATE\":\n",
        "            entities[\"dates\"].append(ent.text)\n",
        "        elif ent.label_ == \"GPE\":\n",
        "            entities[\"locations\"].append(ent.text)\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Function for summarization (dummy implementation)\n",
        "def summarize_text(text):\n",
        "    # In a real scenario, use a summarization model or algorithm\n",
        "    return \"This is a summary of the text snippet.\"\n",
        "\n",
        "# API endpoint\n",
        "@app.post(\"/predict\")\n",
        "async def predict(snippet: Snippet):\n",
        "    text = snippet.text\n",
        "\n",
        "    # Perform multi-label classification\n",
        "    X = vectorizer.transform([text])\n",
        "    predicted_labels = mlb.inverse_transform(model.predict(X))[0]\n",
        "\n",
        "    # Perform entity extraction\n",
        "    extracted_entities = dictionary_lookup(text)\n",
        "    ner_entities = perform_ner(text)\n",
        "\n",
        "    # Generate summary\n",
        "    summary = summarize_text(text)\n",
        "\n",
        "    return {\n",
        "        \"predicted_labels\": predicted_labels,\n",
        "        \"extracted_entities\": {\n",
        "            \"dictionary_lookup\": extracted_entities,\n",
        "            \"ner_entities\": ner_entities\n",
        "        },\n",
        "        \"summary\": summary\n",
        "    }"
      ],
      "metadata": {
        "id": "9YHTVaz5jKEw"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}